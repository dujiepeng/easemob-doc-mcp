# -*- coding: utf-8 -*-
from fastmcp import FastMCP
import os
import sys
import argparse
import asyncio
from typing import List, Dict, Any, Union
from pathlib import Path
from pydantic import Field
from functools import lru_cache, wraps
import json
try:
    from .indexer import global_indexer, build_index_async
except ImportError:
    try:
        from src.indexer import global_indexer, build_index_async
    except ImportError:
        from indexer import global_indexer, build_index_async

# åˆ›å»ºFastMCPå®ä¾‹
mcp = FastMCP()

# --- æ—¥å¿—ç³»ç»Ÿ ---
def log_tool_call(func):
    """å·¥å…·è°ƒç”¨æ—¥å¿—è£…é¥°å™¨ï¼Œè¾“å‡ºåˆ° stderr ä»¥å…å¹²æ‰° stdio ä¼ è¾“"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        tool_name = func.__name__
        # æ‰“å°è¯·æ±‚
        print(f"\n[Tool Request: {tool_name}]", file=sys.stderr)
        print(f"Args: {json.dumps(kwargs, ensure_ascii=False, indent=2)}", file=sys.stderr)
        
        try:
            result = await func(*args, **kwargs)
            # æ‰“å°å“åº”æ‘˜è¦
            print(f"[Tool Response: {tool_name}] Success", file=sys.stderr)
            # å¦‚æœç»“æœå¤ªé•¿ï¼Œåªæ‰“å°å…³é”®ä¿¡æ¯
            res_str = json.dumps(result, ensure_ascii=False)
            if len(res_str) > 500:
                print(f"Result (truncated): {res_str[:500]}...", file=sys.stderr)
            else:
                print(f"Result: {res_str}", file=sys.stderr)
            return result
        except Exception as e:
            print(f"[Tool Response: {tool_name}] Failed: {e}", file=sys.stderr)
            raise e
    return wrapper

# æ–‡æ¡£ä»“åº“é…ç½®
DOC_REPO_URL = os.environ.get("DOC_REPO_URL", "https://github.com/easemob/easemob-doc.git")
DOC_REPO_BRANCH = os.environ.get("DOC_REPO_BRANCH", "doc-v2")

# æ–‡æ¡£æ ¹ç›®å½•
ROOT_DIR = Path(__file__).parent.parent
DOC_ROOT = ROOT_DIR / "document"
UIKIT_ROOT = ROOT_DIR / "uikit"
CALLKIT_ROOT = ROOT_DIR / "callkit"
TEMP_DIR = Path("/tmp/temp_docs")  # ä½¿ç”¨ /tmp é¿å… Docker æŒ‚å·æ—¶çš„æƒé™é—®é¢˜

async def sync_all_docs(force_index: bool = False):
    """åŒæ­¥æ‰€æœ‰æ–‡æ¡£ (é€šè¿‡ä¸‹è½½ zip å‹ç¼©åŒ…å®ç°ï¼Œæ›´è½»å¿«)"""
    import shutil
    import zipfile
    import urllib.request
    import io

    print(f"ğŸš€ å¼€å§‹åŒæ­¥æ–‡æ¡£ä»“åº“ (Archive Mode)...", file=sys.stderr)
    
    # 1. æ¸…ç†å¹¶åˆ›å»ºä¸´æ—¶ç›®å½•
    if TEMP_DIR.exists():
        await asyncio.to_thread(shutil.rmtree, str(TEMP_DIR))
    TEMP_DIR.mkdir(parents=True, exist_ok=True)
    
    try:
        # 2. ä¸‹è½½ ZIP å‹ç¼©åŒ…
        # GitHub Archive URL æ ¼å¼: https://github.com/user/repo/archive/refs/heads/branch.zip
        repo_base = DOC_REPO_URL.replace(".git", "")
        zip_url = f"{repo_base}/archive/refs/heads/{DOC_REPO_BRANCH}.zip"
        
        print(f"ğŸ“¥ Downloading: {zip_url}", file=sys.stderr)
        
        def _download_and_extract():
            with urllib.request.urlopen(zip_url) as response:
                with zipfile.ZipFile(io.BytesIO(response.read())) as z:
                    z.extractall(str(TEMP_DIR))
            
            # è·å–è§£å‹åçš„é¡¶çº§ç›®å½•å (é€šå¸¸æ˜¯ repo-branch)
            top_dirs = [d for d in os.listdir(TEMP_DIR) if os.path.isdir(TEMP_DIR / d)]
            if not top_dirs:
                raise Exception("ZIP è§£å‹åæœªæ‰¾åˆ°ç›®å½•")
            return TEMP_DIR / top_dirs[0]

        extracted_root = await asyncio.to_thread(_download_and_extract)
        print(f"âœ… Extracted to: {extracted_root.name}", file=sys.stderr)

        # 3. å¤åˆ¶å­ç›®å½•
        subfolders = {
            "docs/document": DOC_ROOT,
            "docs/uikit": UIKIT_ROOT,
            "docs/callkit": CALLKIT_ROOT
        }
        
        any_updated = force_index
        for src_rel, dest_path in subfolders.items():
            src_path = extracted_root / src_rel
            if src_path.exists():
                print(f"ğŸ“‚ Updating {dest_path.name}...", file=sys.stderr)
                if dest_path.exists():
                    await asyncio.to_thread(shutil.rmtree, str(dest_path))
                await asyncio.to_thread(shutil.copytree, str(src_path), str(dest_path))
                any_updated = True
            else:
                print(f"âš ï¸ Warning: {src_rel} ä¸å­˜åœ¨äºæºä»£ç ä¸­", file=sys.stderr)
        
        # 4. å¦‚æœæœ‰æ›´æ–°ï¼Œé‡å»ºç´¢å¼•å¹¶æ¸…ç†ç¼“å­˜
        if any_updated:
            print("ğŸ” æ–‡æ¡£æœ‰å˜åŠ¨æˆ–å¼ºåˆ¶æ›´æ–°ï¼Œé‡å»ºç´¢å¼•å¹¶æ¸…ç†ç¼“å­˜...", file=sys.stderr)
            _scan_directory_docs.cache_clear()
            await build_index_async(DOC_ROOT, UIKIT_ROOT, CALLKIT_ROOT, rebuild=True)
        else:
            print("âœ¨ æ–‡æ¡£æ— å˜åŠ¨ï¼Œè·³è¿‡ç´¢å¼•é‡å»ºã€‚", file=sys.stderr)
            
    except Exception as e:
        print(f"âŒ åŒæ­¥è¿‡ç¨‹ä¸­å‡ºé”™: {e}", file=sys.stderr)
    finally:
        # 5. æ¸…ç†ä¸´æ—¶ç›®å½•
        if TEMP_DIR.exists():
            await asyncio.to_thread(shutil.rmtree, str(TEMP_DIR))

async def ensure_docs_ready():
    """å¯åŠ¨æ—¶æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å°±ç»ªï¼Œè‹¥ç¼ºå¤±åˆ™ç«‹å³åŒæ­¥"""
    print("ğŸ“‹ æ£€æŸ¥æœ¬åœ°æ–‡æ¡£å®Œæ•´æ€§...", file=sys.stderr)
    folders = [DOC_ROOT, UIKIT_ROOT, CALLKIT_ROOT]
    missing = any(not (f.exists() and f.is_dir() and any(f.iterdir())) for f in folders)
    
    if missing:
        print("âš ï¸ æ£€æµ‹åˆ°å…³é”®æ–‡æ¡£ç¼ºå¤±ï¼Œæ­£åœ¨å¯åŠ¨ç´§æ€¥åŒæ­¥...", file=sys.stderr)
        await sync_all_docs(force_index=True)
    else:
        print("âœ… æœ¬åœ°æ–‡æ¡£å·²å°±ç»ªã€‚", file=sys.stderr)
        # å³ä½¿æ–‡æ¡£åœ¨ï¼Œä¹Ÿè¦ç¡®ä¿ç´¢å¼•å­˜åœ¨
        from indexer import exists_in
        if not exists_in(global_indexer.index_dir):
            print("ğŸ” ç´¢å¼•ç¼ºå¤±ï¼Œæ­£åœ¨åå°æ„å»ºç´¢å¼•...", file=sys.stderr)
            await build_index_async(DOC_ROOT, UIKIT_ROOT, CALLKIT_ROOT, rebuild=True)

def _read_file_content(path: str) -> str:
    """åŒæ­¥è¯»å–æ–‡ä»¶å†…å®¹"""
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()

@lru_cache(maxsize=128)
def _scan_directory_docs(root_path: Path, platform: str, doc_type: str) -> tuple[List[str], List[str]]:
    """
    åŒæ­¥æ‰«ææ–‡æ¡£ç›®å½•
    """
    results = []
    matched_platforms = []
    
    if not os.path.exists(root_path) or not os.path.isdir(root_path):
        return results, matched_platforms

    is_nested = doc_type in ["uikit", "callkit"]
    prefix_len = len(str(root_path)) + 1

    if is_nested:
        top_dirs = [d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))]
        target_subdirs = []
        
        if not platform:
            target_subdirs = top_dirs
            matched_platforms.append(doc_type)
        else:
            for d in top_dirs:
                if platform in d.lower():
                    target_subdirs.append(d)
                    if doc_type not in matched_platforms:
                        matched_platforms.append(doc_type)
                    continue
                
                sub_path = os.path.join(root_path, d)
                if os.path.exists(sub_path):
                    sub_sub_dirs = [sd for sd in os.listdir(sub_path) if os.path.isdir(os.path.join(sub_path, sd))]
                    for sd in sub_sub_dirs:
                        if platform in sd.lower():
                            target_subdirs.append(d)
                            if doc_type not in matched_platforms:
                                matched_platforms.append(doc_type)
                            break
        
        target_subdirs = list(set(target_subdirs))
        
        for subdir in target_subdirs:
            subdir_path = os.path.join(root_path, subdir)
            for root, _, files in os.walk(subdir_path):
                if platform:
                    rel_to_subdir = os.path.relpath(root, subdir_path)
                    if rel_to_subdir != ".":
                        first_part = rel_to_subdir.split(os.sep)[0].lower()
                        if first_part != platform:
                            continue

                for file in files:
                    if file.endswith('.md'):
                        full_path = os.path.join(root, file)
                        rel_path = full_path[prefix_len:]
                        results.append(f"{doc_type}/{rel_path}")

        if not platform:
            for file in os.listdir(root_path):
                if file.endswith('.md'):
                    results.append(f"{doc_type}/{file}")

    else:
        top_dirs = [d for d in os.listdir(root_path) if os.path.isdir(os.path.join(root_path, d))]
        target_dirs = []
        
        if not platform:
            target_dirs = top_dirs
            matched_platforms.extend(top_dirs)
        else:
            target_dirs = [d for d in top_dirs if platform in d.lower()]
            matched_platforms.extend(target_dirs)
            
        for d in target_dirs:
            dir_path = os.path.join(root_path, d)
            for root, _, files in os.walk(dir_path):
                for file in files:
                    if file.endswith('.md'):
                        full_path = os.path.join(root, file)
                        rel_path = full_path[prefix_len:]
                        results.append(rel_path)

    return results, matched_platforms

@mcp.tool()
@log_tool_call
async def search_platform_docs(
    doc_type: str = Field(
        default="sdk",
        description="æ–‡æ¡£ç±»å‹ï¼Œå¿…å¡«å‚æ•°ï¼Œå¯é€‰å€¼ä¸º 'sdk'ã€'uikit' æˆ– 'callkit'"
    ),
    platform: str = Field(
        default="",
        description="å¹³å°åç§°ï¼Œå¦‚androidã€iosç­‰"
    )
) -> Dict[str, Any]:
    """
    æœç´¢ç‰¹å®šå¹³å°çš„æ–‡æ¡£ç›®å½• (ä¼ ç»Ÿæ–‡ä»¶ååˆ—è¡¨æœç´¢)ã€‚
    """
    try:
        if doc_type.lower() not in ["sdk", "uikit", "callkit"]:
            return {"documents": [], "error": f"æ— æ•ˆçš„æ–‡æ¡£ç±»å‹: {doc_type}"}
            
        normalized_doc_type = doc_type.lower()
        normalized_platform = platform.lower()
        
        mapping = {}
        if normalized_doc_type == "uikit":
            mapping = {
                "å°ç¨‹åº": "applet", "uni-app": "uniapp", "é¸¿è’™": "harmonyos", 
                "rn": "react-native", "rest": "server-side"
            }
        else:
            mapping = {
                "å°ç¨‹åº": "applet", "uni-app": "applet", "é¸¿è’™": "harmonyos", 
                "rn": "react-native", "rest": "server-side"
            }
        
        for key, value in mapping.items():
            if key in normalized_platform:
                normalized_platform = value
                break
        
        root_path = DOC_ROOT
        if normalized_doc_type == "uikit":
            root_path = UIKIT_ROOT
        elif normalized_doc_type == "callkit":
            root_path = CALLKIT_ROOT
            
        documents, matched_platforms = await asyncio.to_thread(
            _scan_directory_docs, root_path, normalized_platform, normalized_doc_type
        )
        
        result_platform = matched_platforms[0] if len(matched_platforms) == 1 else platform

        return {
            "documents": documents,
            "platform": result_platform,
            "count": len(documents),
            "error": None
        }

    except Exception as e:
        return {"documents": [], "error": f"æœç´¢æ–‡æ¡£é”™è¯¯: {str(e)}"}

@mcp.tool()
@log_tool_call
async def get_document_content(
    doc_paths: Union[str, List[str]] = Field(
        default=None,
        description="æ–‡æ¡£ç›¸å¯¹è·¯å¾„åˆ—è¡¨ï¼Œä¾‹å¦‚ [\"sdk/android/quickstart.md\", \"uikit/chatuikit/android/chatuikit_quickstart.md\"]ï¼Œæˆ–è€…å•ä¸ªå­—ç¬¦ä¸²è·¯å¾„"
    ),
    keyword: str = Field(
        default="",
        description="å¯é€‰ï¼Œç”¨äºåœ¨è¿”å›çš„æ–‡æ¡£å†…å®¹ä¸­è¿›è¡Œå…³é”®è¯å®šä½ï¼ˆè¿”å›ä¸Šä¸‹æ–‡ï¼‰"
    )
) -> Dict[str, Any]:
    """è·å–æ–‡æ¡£å†…å®¹"""
    try:
        if doc_paths is None:
            doc_paths = []
        elif isinstance(doc_paths, str):
            doc_paths = [doc_paths]
        
        results = []
        total_matches = 0
        
        for doc_path in doc_paths:
            try:
                if doc_path.startswith("uikit/"):
                    full_path = UIKIT_ROOT / doc_path[6:]
                elif doc_path.startswith("callkit/"):
                    full_path = CALLKIT_ROOT / doc_path[8:]
                elif doc_path.startswith("sdk/"):
                    full_path = DOC_ROOT / doc_path[4:]
                else:
                    full_path = DOC_ROOT / doc_path
                
                if not full_path.exists():
                    results.append({"error": "æ–‡æ¡£ä¸å­˜åœ¨", "docPath": doc_path})
                    continue
                
                content = await asyncio.to_thread(_read_file_content, str(full_path))
                content = content.replace('\t', '')
                matches = []
                
                if keyword and keyword.strip() != "":
                    lines = content.split('\n')
                    keyword_lower = keyword.lower()
                    for i, line in enumerate(lines):
                        line_no_tabs = line.replace('\t', '')
                        if keyword_lower in line_no_tabs.lower():
                            start = max(0, i - 2)
                            end = min(len(lines) - 1, i + 2)
                            context = '\n'.join([lines[j].replace('\t', '') for j in range(start, end + 1)])
                            matches.append({
                                "lineNumber": i + 1,
                                "context": context,
                                "line": line_no_tabs
                            })
                
                results.append({
                    "content": content,
                    "docPath": doc_path,
                    "matches": matches
                })
                total_matches += len(matches)
                
            except Exception as e:
                results.append({"error": str(e), "docPath": doc_path})
        
        return {"documents": results, "totalMatches": total_matches}
    except Exception as e:
        return {"error": str(e)}

# @mcp.tool()
# @log_tool_call
async def search_knowledge_base(
    query: str = Field(description="è‡ªç„¶è¯­è¨€æœç´¢æŸ¥è¯¢ï¼Œä¾‹å¦‚ 'å¦‚ä½•é›†æˆç¯ä¿¡ IM' æˆ– 'login error'"),
    doc_type: str = Field(default=None, description="å¯é€‰ï¼Œè¿‡æ»¤æ–‡æ¡£ç±»å‹: 'sdk', 'uikit', 'callkit'"),
    platform: str = Field(default=None, description="å¯é€‰ï¼Œè¿‡æ»¤å¹³å°: 'android', 'ios' ç­‰")
) -> List[Dict[str, Any]]:
    """
    å…¨æ–‡æ£€ç´¢çŸ¥è¯†åº“ (åŸºäºæœç´¢å¼•æ“)ã€‚
    æ”¯æŒè‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œè¿”å›æŒ‰ç›¸å…³æ€§æ’åºçš„æ–‡æ¡£åˆ—è¡¨ï¼ŒåŒ…å«åŒ¹é…é«˜äº®æ‘˜è¦ã€‚
    æ¨èåœ¨ç”¨æˆ·æå‡ºæ¨¡ç³Šé—®é¢˜æ—¶ä½¿ç”¨ã€‚
    """
    # å¼‚æ­¥æ‰§è¡Œæœç´¢
    results = await asyncio.to_thread(global_indexer.search, query, 10, doc_type, platform)
    return results

def main():
    parser = argparse.ArgumentParser(description="ç¯ä¿¡æ–‡æ¡£æœç´¢ MCP æœåŠ¡")
    parser.add_argument("--transport", "-t", choices=["stdio", "http", "sse"], default="http")
    parser.add_argument("--host", default="0.0.0.0")
    parser.add_argument("--port", "-p", type=int, default=443)
    parser.add_argument("--path", default="/sse")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–ç´¢å¼• (åœ¨å¯åŠ¨æ—¶)
    # åªèƒ½åœ¨äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ async å‡½æ•°ï¼Œä½† mcp.run ä¼šæ¥ç®¡äº‹ä»¶å¾ªç¯
    # æ‰€ä»¥æˆ‘ä»¬åœ¨ main ä¸­å…ˆè¿è¡Œä¸€æ¬¡ç´¢å¼•æ„å»ºï¼ˆåŒæ­¥é˜»å¡æ–¹å¼ï¼Œæˆ–è€… fire-and-forgetï¼‰
    # ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ asyncio.run æ¥æ‰§è¡Œç´¢å¼•æ„å»ºï¼Œç„¶åå†å¯åŠ¨ MCP
    
    print("æ­£åœ¨åŠ è½½ç¯ä¿¡æ–‡æ¡£ MCP æœåŠ¡...", file=sys.stderr)
    try:
        # 1. å¯åŠ¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡æ¡£å­˜ä¸”å·²å»ºç«‹ç´¢å¼•
        asyncio.run(ensure_docs_ready())
        
        # å¯åŠ¨åå°å®šæ—¶æ›´æ–°ä»»åŠ¡
        async def scheduled_update():
            while True:
                # æ¯å¤©æ›´æ–°ä¸€æ¬¡ (24 * 3600 ç§’)
                # ä¸ºäº†ä¾¿äºæµ‹è¯•ï¼Œå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®é—´éš”ï¼Œé»˜è®¤ 86400 ç§’
                try:
                    update_interval = int(os.environ.get("DOC_UPDATE_INTERVAL_SECONDS", 86400))
                    print(f"ä¸‹æ¬¡æ–‡æ¡£æ›´æ–°å°†åœ¨ {update_interval} ç§’åæ‰§è¡Œ...", file=sys.stderr)
                    await asyncio.sleep(update_interval)
                    
                    print("â° å¼€å§‹æ‰§è¡Œå®šæ—¶æ›´æ–°...", file=sys.stderr)
                    # æ‰§è¡Œå…¨é‡æ›´æ–°
                    await sync_all_docs()
                        
                except Exception as e:
                    print(f"å®šæ—¶æ›´æ–°ä»»åŠ¡å‡ºé”™: {e}", file=sys.stderr)
                    await asyncio.sleep(60) # å‡ºé”™åç­‰å¾… 1 åˆ†é’Ÿé‡è¯•

        # åœ¨åå°å¯åŠ¨ä»»åŠ¡ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
        import threading
        def run_schedule():
            asyncio.run(scheduled_update())
        
        # æ³¨æ„: mcp.run() æ˜¯é˜»å¡çš„ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ç®€å•çš„çº¿ç¨‹æˆ–è€…åœ¨ mcp å†…éƒ¨æœºåˆ¶ä¸­å¯åŠ¨
        # FastMCP ç›®å‰å¯ä»¥ç›´æ¥è¿è¡Œï¼Œæˆ‘ä»¬æŠŠ asyncio task æ”¾åˆ°äº‹ä»¶å¾ªç¯é‡Œæœ€å¥½
        # ä½†ç”±äº mcp.run() ä¼šæ¥ç®¡å¾ªç¯ï¼Œæˆ‘ä»¬è¿™é‡Œç”¨ä¸€ä¸ªç®€åŒ–çš„æ–¹å¼ï¼š
        # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ¥è¿è¡Œè¿™ä¸ªç‹¬ç«‹çš„ loop (è™½ç„¶ä¸æ˜¯æœ€ä½³å®è·µï¼Œä½†å¯¹ç®€å•ä»»åŠ¡æœ‰æ•ˆ)
        # æˆ–è€…ï¼Œå¦‚æœ FastMCP æš´éœ²äº† startup hook æ›´å¥½ã€‚
        # é‰´äº FastMCP å°è£…è¾ƒæ·±ï¼Œæˆ‘ä»¬ç®€å•åœ°ç”¨ threading å¯åŠ¨è¿™ä¸ª loop
        t = threading.Thread(target=run_schedule, daemon=True)
        t.start()
        
    except Exception as e:
        print(f"ç´¢å¼•æ„å»ºå¤±è´¥: {e}", file=sys.stderr)
        print("æœåŠ¡å°†ç»§ç»­è¿è¡Œï¼Œä½†æœç´¢åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨ã€‚", file=sys.stderr)
    
    print(f"å¯åŠ¨ç¯ä¿¡æ–‡æ¡£æœç´¢MCPæœåŠ¡å™¨ (v1.1.11 - Full Text Search)", file=sys.stderr)
    if args.transport == "stdio":
        mcp.run(transport="stdio")
    elif args.transport == "sse":
        mcp.run(transport="sse", host=args.host, port=args.port, path=args.path)
    else:
        mcp.run(transport="http", host=args.host, port=args.port, path=args.path)

if __name__ == "__main__":
    main()
